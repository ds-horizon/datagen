package main

import (
	"fmt"
	"log/slog"

	"github.com/IBM/sarama"
)

// Load___datagen_{{.FullyQualifiedModelName}}_kafka sends a batch of records to the configured Kafka topic.
func Load___datagen_{{.FullyQualifiedModelName}}_kafka(records []*__datagen_{{.FullyQualifiedModelName}}, config *__dgi_KafkaConfig) error {
	if len(records) == 0 {
		return nil
	}

	producer, err := Get___datagen_{{.FullyQualifiedModelName}}_kafka_producer()
	if err != nil {
		return fmt.Errorf("failed to get kafka producer: %w", err)
	}

	for _, record := range records {
		// Serialize the record to JSON (default serialization)
		valueBytes := record.__dgi_Serialise()

		// Prepare the Kafka message
		msg := &sarama.ProducerMessage{
			Topic: config.Topic,
			Value: sarama.ByteEncoder(valueBytes),
		}

		// Set the key if provided in config
		if config.Key != "" {
			msg.Key = sarama.StringEncoder(config.Key)
		}

		// Send the message synchronously
		partition, offset, err := producer.SendMessage(msg)
		if err != nil {
			return fmt.Errorf("failed to send message to kafka: %w", err)
		}

        	slog.Debug(fmt.Sprintf("sent kafka message on partition %d at offset %d", partition, offset))
	}

	return nil
}

// Truncate___datagen_{{.FullyQualifiedModelName}}_kafka is a no-op for Kafka as it's an append-only log.
func Truncate___datagen_{{.FullyQualifiedModelName}}_kafka(config *__dgi_KafkaConfig) error {
	return nil
}

