package main

import (
	"fmt"

	"github.com/IBM/sarama"
)

// Load___datagen_{{.FullyQualifiedModelName}}_kafka sends a batch of records to the configured Kafka topic.
func Load___datagen_{{.FullyQualifiedModelName}}_kafka(records []*__datagen_{{.FullyQualifiedModelName}}, config *__dgi_KafkaConfig) error {
	if len(records) == 0 {
		return nil
	}

	producer, err := Get___datagen_{{.FullyQualifiedModelName}}_kafka_producer()
	if err != nil {
		return fmt.Errorf("failed to get kafka producer: %w", err)
	}

	for _, record := range records {
		// Serialize the record to JSON (default serialization)
		valueBytes := record.__dgi_Serialise()

		// Prepare the Kafka message
		msg := &sarama.ProducerMessage{
			Topic: config.Topic,
			Value: sarama.ByteEncoder(valueBytes),
		}

		// Set the key if provided in config
		if config.Key != "" {
			msg.Key = sarama.StringEncoder(config.Key)
		}

		// Send the message synchronously
		partition, offset, err := producer.SendMessage(msg)
		if err != nil {
			return fmt.Errorf("failed to send message to kafka: %w", err)
		}

		// Optional: Log successful send (can be removed or made conditional)
		_ = partition
		_ = offset
	}

	return nil
}

// Truncate___datagen_{{.FullyQualifiedModelName}}_kafka is a no-op for Kafka as it's an append-only log.
// Kafka topics cannot be truncated in the traditional sense without deleting and recreating them.
func Truncate___datagen_{{.FullyQualifiedModelName}}_kafka(config *__dgi_KafkaConfig) error {
	// No-op: Kafka doesn't support truncation like SQL databases
	// If you need to clear data, you would need to delete and recreate the topic,
	// which requires admin permissions and is typically not done during normal operations.
	return nil
}

